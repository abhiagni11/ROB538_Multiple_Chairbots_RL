{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "# torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import colors\n",
    "\n",
    "class Restaurant:\n",
    "\n",
    "\tdef __init__(self, number_of_tables, number_of_agents, grid_dim_x, grid_dim_y, number_of_steps):\n",
    "\t\tself._number_of_tables = number_of_tables\n",
    "\t\tself._number_of_agents = number_of_agents\n",
    "\t\tself._Ny = grid_dim_y  # y grid size\n",
    "\t\tself._Nx = grid_dim_x  # x grid size\n",
    "\t\tself._number_of_steps = number_of_steps\n",
    "\t\tself._step = 0\n",
    "\t\tself._all_tables = dict() # (x, y, max_capacity)\n",
    "\t\tself._all_agents = dict() # (previousX, previousY, X, Y, group_of_people)\n",
    "\t\tself._groups_of_people = dict() # (X, Y, number_of_people)\n",
    "\t\tself._number_of_groups_present = 0\n",
    "\t\tself._max_number_of_groups = 10\n",
    "\t\tself._agent_rewards = [0] * self._number_of_agents\n",
    "\t\tself._system_reward = 0\n",
    "\t\tself._action_dim = (5,)  # up, right, down, left\n",
    "\t\tself._action_dict = {\"none\": 0, \"up\": 1, \"right\": 2, \"down\": 3, \"left\": 4}\n",
    "\t\tself._action_coords = [(0, 0), (-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "\t\tself.initialise_tables()\n",
    "\t\tself.initialise_agents()\n",
    "\t\tself.initialise_groups()\n",
    "\t\tself.fig, self.ax = plt.subplots()\n",
    "\t\tself._grid_viz = [[0 for x in range(self._Nx + 2)] for x in range(self._Ny + 2)]\n",
    "\t\tself.initialise_visualization()\n",
    "\n",
    "\tdef initialise_tables(self):\n",
    "\t\tfor elem in range(self._number_of_tables):\n",
    "\t\t\tself._all_tables[elem] = (randint(0, self._Nx), randint(0, self._Ny), randint(2,2))\n",
    "\n",
    "\tdef initialise_agents(self):\n",
    "\t\tfor elem in range(self._number_of_agents):\n",
    "\t\t\t# self._all_agents[elem] = (0, 0, self._all_tables[elem][0], self._all_tables[elem][1], -1)\n",
    "\t\t\tself._all_agents[elem] = (0, 0, randint(0, self._Nx), randint(0, self._Ny), -1)\n",
    "\t\t\tself._agent_rewards[elem] = 0\n",
    "\n",
    "\tdef initialise_groups(self):\n",
    "\t\tfor elem in range(self._max_number_of_groups):\n",
    "\t\t\tself._groups_of_people[elem] = (-1, -1, randint(2,2))\n",
    "\n",
    "\tdef initialise_visualization(self):\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.imshow(self._grid_viz, cmap='Greys')\n",
    "\t\tplt.ion()\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself._all_tables.clear()\n",
    "\t\tself._all_agents.clear()\n",
    "\t\tself._agent_rewards = [0] * self._number_of_agents\n",
    "\t\tself._groups_of_people.clear()\n",
    "\t\tself.initialise_tables()\n",
    "\t\tself.initialise_agents()\n",
    "\t\tself.initialise_groups()\n",
    "\n",
    "\tdef step(self, all_agent_actions):\n",
    "\t\tself._agent_rewards = [0] * self._number_of_agents\n",
    "\n",
    "\t\tfor elem, each_agent in (self._all_agents.items()):\n",
    "\t\t\t# Get agent's next state\n",
    "\t\t\tagent_state_next = (each_agent[2] + self._action_coords[all_agent_actions[elem]][0], each_agent[3] + self._action_coords[all_agent_actions[elem]][1])\n",
    "\t\t\t# Update the state and reward of each agent\n",
    "\t\t\tself.update_state_and_reward(elem, each_agent, agent_state_next)\n",
    "\t\tself._step += 1\n",
    "\n",
    "\t\t# Introduce people in the system\n",
    "\t\tself.add_new_group_of_people()\n",
    "\n",
    "\t\t# Calculate the system reward\n",
    "\t\tself.calculate_system_reward()\n",
    "\n",
    "\tdef update_state_and_reward(self, elem, each_agent, agent_state_next):\n",
    "\t\t# check if agent has any group with it\n",
    "\t\tif each_agent[-1] is not -1:\n",
    "\t\t\tgroup_number = each_agent[-1]\n",
    "\t\t\t# it had a group, so update that group's location according to the agent's updated state and check if the group reached any table\n",
    "\t\t\treached_table_number = self.update_group_location(group_number, agent_state_next)\n",
    "\t\t\t# Agent's reward is -1 for carrying the group with it\n",
    "\t\t\tself._agent_rewards[elem] -= 1\n",
    "\t\t\tif reached_table_number is not -1:\n",
    "\t\t\t\tself._agent_rewards[elem] += 40\n",
    "\t\telse:\n",
    "\t\t\t# check if it just stumbled into a group or not\n",
    "\t\t\tgroup_number = self.check_agent_found_group(agent_state_next)\n",
    "\t\tif group_number is not -1:\n",
    "\t\t\t# Agent just found a group, so it'll get +40 reward\n",
    "\t\t\tself._agent_rewards[elem] += 40\n",
    "\t\t# Update the agent's reward in the dictionary\n",
    "\t\tself._all_agents[elem] = (each_agent[2], each_agent[3], agent_state_next[0], agent_state_next[1], group_number)\n",
    "\t\t\n",
    "\n",
    "\tdef update_group_location(self, group_number, location):\n",
    "\t\t# Update the particular group's location\n",
    "\t\tself._groups_of_people[group_number][0] = location[0]\n",
    "\t\tself._groups_of_people[group_number][1] = location[1]\n",
    "\t\treturn self.check_group_reached_which_table(group_number)\n",
    "\n",
    "\tdef check_agent_found_group(self, state):\n",
    "\t\t# iterate through all the groups' locations and find a match with the agent's updated state\n",
    "\t\t# returns the group id if agent finds a group, otherwise return -1\n",
    "\t\tgroup = -1\n",
    "\t\tfor elem, each_group in self._groups_of_people.items():\n",
    "\t\t\tif state[0] == (each_group[0]) and state[1] == each_group[1]:\n",
    "\t\t\t\tgroup = elem\n",
    "\t\t\t\tbreak\n",
    "\t\treturn group\n",
    "\n",
    "\tdef check_group_reached_which_table(self, group_number):\n",
    "\t\treached_table_number = -1\n",
    "\t\tgroup = self._groups_of_people[group_number]\n",
    "\t\tfor elem, each_table in self._all_tables.values():\n",
    "\t\t\tif group[0] == each_table[0] and group[1] == each_table[1]:\n",
    "\t\t\t\treached_table = elem\n",
    "\t\t\t\t(self._groups_of_people[group_number][0], self._groups_of_people[group_number][1]) = (0, 0)\n",
    "\t\t\t\tbreak\n",
    "\t\treturn reached_table_number\n",
    "\n",
    "\tdef add_new_group_of_people(self):\n",
    "\t\titer_step = self._number_of_steps/100\n",
    "\t\t# if ((self._step < (iter_step * 10)) and (self._step % iter_step == 0)):\n",
    "\t\t# \tself._groups_of_people[self._number_of_groups_present] = (0, 0, randint(2,2))\n",
    "\t\t# \tself._number_of_groups_present += 1\n",
    "\t\tif (self._step == 0):\n",
    "\t\t\tself._groups_of_people[self._number_of_groups_present] = (0, 0, randint(2,2))\n",
    "\t\t\tself._number_of_groups_present += 1\n",
    "\n",
    "\tdef calculate_system_reward(self):\n",
    "\t\tself._system_reward = 0\n",
    "\t\tfor reward in self._agent_rewards:\n",
    "\t\t\tself._system_reward += reward\n",
    "\n",
    "\tdef get_system_reward(self):\n",
    "\t\treturn self._system_reward\n",
    "\n",
    "\tdef get_observation(self):\n",
    "\t\treturn self._all_tables, self._all_agents, self._groups_of_people\n",
    "\n",
    "\tdef get_local_reward(self, agent):\n",
    "\t\tNone\n",
    "\n",
    "\tdef get_local_observation(self, agent):\n",
    "\t\tNone\n",
    "\n",
    "\tdef visualize_restaurant(self):\n",
    "\t\t\"\"\"Visualize the grid with the agents and target.\n",
    "\t\t\"\"\" \n",
    "\t\tplt.cla()\n",
    "\t\t# self.ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "\t\t# self.ax.set_xticks(np.arange(0, self._Nx+1, 5));\n",
    "\t\t# self.ax.set_yticks(np.arange(0, self._Ny+1, 5));\n",
    "\t\tself.ax.set_xlim([0, self._Nx+2])\n",
    "\t\tself.ax.set_ylim([0, self._Ny+2])\n",
    "\n",
    "\t\t# Plot tables\n",
    "\t\tfor elem, each_table in (self._all_tables.items()):\n",
    "\t\t\ttable_state = (each_table[0], each_table[1])\n",
    "\t\t\tself.ax.plot(table_state[0]+1, table_state[1]+1, \"cs\", markersize=10)\n",
    "\t\t\ttable_text = 'T' + str(elem)\n",
    "\t\t\tself.ax.text(table_state[0] -.75+1, table_state[1] -.5+1, table_text)\n",
    "\n",
    "\t\t# Plot agents\n",
    "\t\tfor elem, each_agent in (self._all_agents.items()):\n",
    "\t\t\tagent_state = (each_agent[2], each_agent[3])\n",
    "\t\t\tchairbot = plt.Circle((agent_state[0]+1, agent_state[1]+1), 1, color='y')\n",
    "\t\t\tself.ax.add_artist(chairbot)\n",
    "\t\t\tchairbot_text = 'C' + str(elem)\n",
    "\t\t\tself.ax.text(agent_state[0] -.75+1, agent_state[1] -.5+1, chairbot_text)\n",
    "\t\t\n",
    "\t\t# Plot entrance\n",
    "\t\tcircle_entrance = plt.Circle((0+1, 0+1), 1, color='g')\n",
    "\t\tself.ax.add_artist(circle_entrance)\n",
    "\t\tself.ax.text(-10, .5, 'ENTRANCE ->')\n",
    "\n",
    "\t\t# Plot reward\n",
    "\t\treward_text = 'R-' + str(self._system_reward)\n",
    "\t\tself.ax.text(-10, self._Ny, reward_text)\n",
    "\n",
    "\t\tself.ax.plot()\n",
    "\t\tplt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\t\t# plt.axis('off')\n",
    "\t\tplt.draw()\n",
    "\t\tplt.pause(.01)\n",
    "\n",
    "\n",
    "def concatenate_state(all_tables, all_agents, all_groups_of_people):\n",
    "\tstate = []\n",
    "\tfor elem, each_table in (all_tables.items()):\n",
    "\t\tstate.extend(each_table)\n",
    "\tfor elem, each_agent in (all_agents.items()):\n",
    "\t\tstate.extend(each_agent)\n",
    "\tfor elem, each_group in (all_groups_of_people.items()):\n",
    "\t\tstate.extend(each_group)\n",
    "\tstate_array = np.array(state)\n",
    "\treturn state_array\n",
    "\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "\tdef __init__(self, number_of_tables, number_of_agents, grid_dim_x, grid_dim_y):\n",
    "\t\tself._number_of_tables = number_of_tables\n",
    "\t\tself._number_of_agents = number_of_agents\n",
    "\t\tself._Ny = grid_dim_y  # y grid size\n",
    "\t\tself._Nx = grid_dim_x  # x grid size\n",
    "\t\tself._action = 0  # x grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DQN\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "    def __init__(self,n_feature = 38,n_hidden = 25,n_hidden_1 = 30,n_hidden_2 = 30,n_output = 5):\n",
    "        super(DQN, self).__init__()\n",
    "     \n",
    "        self._n_feature = n_feature\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        \n",
    "        self.hidden=torch.nn.Linear(self._n_feature, self.n_hidden)   # hidden layer\n",
    "        init.xavier_normal(self.hidden.weight, gain=np.sqrt(2))\n",
    "        self.dropout= torch.nn.Dropout(p=0.3)\n",
    "    \n",
    "        self.hidden1= torch.nn.Linear(self.n_hidden, self.n_hidden_1)   # hidden layer\n",
    "        init.xavier_normal(self.hidden1.weight, gain=np.sqrt(2))\n",
    "        self.dropout1=torch.nn.Dropout(p=0.3)      \n",
    "    \n",
    "        self.hidden2= torch.nn.Linear(self.n_hidden_1, self.n_hidden_2)   # hidden layer\n",
    "        init.xavier_normal(self.hidden2.weight, gain=np.sqrt(2))\n",
    "        self.dropout2=torch.nn.Dropout(p=0.3)      \n",
    "        \n",
    "        self.predict=torch.nn.Linear(self.n_hidden_2, self.n_output)   # output layer. if hidden 2 is used\n",
    "    \n",
    "#         self.predict=torch.nn.Linear(self._n_hidden_1, self.n_output)   # output layer. if hidden1 is last hidden layer\n",
    "        init.xavier_normal(self.predict.weight, gain=np.sqrt(2))\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.hidden(x)) \n",
    "        x=self.dropout(x)\n",
    "        x=F.relu(self.hidden1(x)) \n",
    "        x=self.dropout1(x)        \n",
    "        x=F.relu(self.hidden2(x)) \n",
    "        x=self.dropout2(x)        \n",
    "        x=self.predict(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(policy_net_output):\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * (math.exp(- 1 * steps_done / eps_decay))\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "#                 return self._policy_net(self._state).max(1)[1].view(1, 1)\n",
    "            return policy_net_output.max(1)[1].view(1,1)\n",
    "    else:\n",
    "#         return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "        return torch.tensor([[random.randrange(2)]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_network_eval(policy_net,state):\n",
    "    return policy_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model(policy_net,target_net,memory,optimizer,gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "# Compute a mask of non-final states and concatenate the batch elements\n",
    "\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.uint8)\n",
    "    \n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    \n",
    "#     print(\"state_batch\",type(state_batch),)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1,action_batch)\n",
    "#     state_action_values = state_action_values_temp.gather(1,action_batch)\n",
    "    #     .gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values = torch.zeros(batch_size) \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "#     next_state_values.double()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) #+ reward_batch.double()\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    ### Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "#     print(\"step-done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarly define networks for all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risheek/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:15: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/risheek/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:19: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/risheek/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/risheek/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
     ]
    }
   ],
   "source": [
    "policy_net_agent_1 = DQN()\n",
    "policy_net_agent_1.double()\n",
    "# policy_net.cuda()\n",
    "target_net_agent_1 = DQN()\n",
    "target_net_agent_1.double()\n",
    "# target_net.cuda()\n",
    "target_net_agent_1.load_state_dict(policy_net_agent_1.state_dict())\n",
    "# target_net_agent_1.eval()\n",
    "\n",
    "\n",
    "optimizer_agent_1 = optim.RMSprop(policy_net_agent_1.parameters())\n",
    "memory= ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "gamma = 0.999\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 200\n",
    "batch_size = 10\n",
    "capacity = 10000\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net_agent_1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a1 = torch.DoubleTensor(np.array([1, 2,2,3,4,5,6,7,8,9], dtype=np.float64)) # works\n",
    "# print(policy_network_eval(policy_net_agent_1,a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# steps_done = 0\n",
    "# episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# memory = ReplayMemory(10000)\n",
    "\n",
    "# states = np.random.rand(100,10)\n",
    "# # print(states.shape)\n",
    "# action = np.random.randint(0,4,size=[100,1])\n",
    "# # print(action.shape)\n",
    "# reward = np.random.randint(0,10,size=[100,1])\n",
    "# next_states = np.random.rand(100,10)\n",
    "\n",
    "# for i in range(states.shape[0]):\n",
    "# #     memory.push(states[i],action[i],next_states[i],reward[i])\n",
    "#     memory.push(torch.from_numpy(states[i]).unsqueeze(0),torch.from_numpy(action[i]).unsqueeze(0),torch.from_numpy(next_states[i]).unsqueeze(0),torch.from_numpy(reward[i]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_model(policy_net_agent_1,target_net_agent_1,memory,optimizer,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16f5088cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (38,) <class 'numpy.ndarray'> [ 18.  16.   2.   0.   0.  22.  27.  -1.  -1.  -1.   2.  -1.  -1.   2.  -1.\n",
      "  -1.   2.  -1.  -1.   2.  -1.  -1.   2.  -1.  -1.   2.  -1.  -1.   2.  -1.\n",
      "  -1.   2.  -1.  -1.   2.  -1.  -1.   2.]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.DoubleTensor for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5b3a92768981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;31m# actions_of_all_agents[elem] = each_agent.get_action(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#                   actions_of_all_agents[elem] = randint(0,4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mpolicy_net_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net_agent_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0mactions_of_all_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-59b45d0fa3e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.DoubleTensor for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    number_of_tables = 1\n",
    "    number_of_agents = 1\n",
    "    grid_dim_x = 40\n",
    "    grid_dim_y = 40\n",
    "\n",
    "    number_of_training_loops = 1\n",
    "    number_of_episodes = 1\n",
    "    number_of_steps = 100\n",
    "\n",
    "    # main()\n",
    "\n",
    "    restaurant = Restaurant(number_of_tables, number_of_agents, grid_dim_x, grid_dim_y, number_of_steps)\n",
    "    all_the_agents = dict()\n",
    "\n",
    "    for elem in range(number_of_agents):\n",
    "        agent = Agent(number_of_tables, number_of_agents, grid_dim_x, grid_dim_y)\n",
    "        all_the_agents[elem] = agent._action\n",
    "\n",
    "    for training_loop in range(number_of_training_loops):\n",
    "        steps_done = training_loop\n",
    "        for episode in range(number_of_episodes):\n",
    "            \n",
    "            # reset the environment\n",
    "            restaurant.reset()\n",
    "\n",
    "            # Get initial observation from the enivorment\n",
    "            all_table, all_agents, all_people = restaurant.get_observation()\n",
    "            state = concatenate_state(all_table, all_agents, all_people)\n",
    "            state = state.astype(float)\n",
    "            print('state',state.shape,type(state),state)\n",
    "            for each_step in range(number_of_steps):\n",
    "                \n",
    "                # get actions from all the agents\n",
    "                actions_of_all_agents = [0] * number_of_agents\n",
    "                actions_of_all_agents = np.asarray(actions_of_all_agents)\n",
    "                for elem, each_agent in all_the_agents.items():\n",
    "                    # actions_of_all_agents[elem] = each_agent.get_action(state)\n",
    "#                   actions_of_all_agents[elem] = randint(0,4)\n",
    "                    policy_net_output = policy_net_agent_1(torch.from_numpy(state))\n",
    "                    actions_of_all_agents[elem] = get_action(policy_net_output)\n",
    "                \n",
    "                # Step: get next state and reward for the action taken\n",
    "                restaurant.step(actions_of_all_agents)\n",
    "\n",
    "                # Get updated observation from the environment\n",
    "                next_all_table, next_all_agents, next_all_people = restaurant.get_observation()\n",
    "                next_state = concatenate_state(next_all_table, next_all_agents, next_all_people)\n",
    "\n",
    "                # Get reward for the action taken\n",
    "                system_reward = restaurant.get_system_reward()\n",
    "\n",
    "                # Store the transition in memory\n",
    "                memory.push(state, actions_of_all_agents[0], next_state, system_reward)\n",
    "\n",
    "                # modify state for the next step\n",
    "                state = next_state\n",
    "#               restaurant.visualize_restaurant()\n",
    "#               time.sleep(0.001)   \n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        None\n",
    "        # optimize_model()\n",
    "        optimize_model(policy_net_agent_1,target_net_agent_1,memory,optimizer,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
